{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#2048 Deep Q-Learning implementation\n",
    "\n",
    "This Google Colab Jupyter Notebook consists of the game source code and fully implemented deep q-learning neural network designed to win it\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial game conditions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "game_board = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial board: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "Seeded board: [[0, 0, 0, 0], [0, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "[[0, 0, 0, 0], [0, 0, 0, 2], [0, 0, 0, 0], [0, 2, 0, 0]]\n",
      "Score: 0\n",
      "Seeded board: [[0, 0, 0, 0], [0, 0, 0, 2], [0, 0, 0, 0], [0, 2, 0, 0]]\n",
      "[[0, 0, 0, 0], [2, 0, 0, 0], [0, 0, 2, 0], [2, 0, 0, 0]]\n",
      "Score: 0\n",
      "Seeded board: [[0, 0, 0, 0], [2, 0, 0, 0], [0, 0, 2, 0], [2, 0, 0, 0]]\n",
      "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 2], [4, 0, 2, 0]]\n",
      "Score: 4\n",
      "Seeded board: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 2], [4, 0, 2, 0]]\n",
      "[[0, 0, 0, 0], [0, 0, 0, 0], [2, 0, 0, 2], [4, 2, 0, 0]]\n",
      "Score: 4\n",
      "Seeded board: [[0, 0, 0, 0], [0, 0, 0, 0], [2, 0, 0, 2], [4, 2, 0, 0]]\n",
      "[[0, 0, 0, 2], [0, 0, 0, 0], [2, 0, 0, 0], [4, 2, 0, 2]]\n",
      "Score: 4\n",
      "Seeded board: [[0, 0, 0, 2], [0, 0, 0, 0], [2, 0, 0, 0], [4, 2, 0, 2]]\n",
      "[[2, 0, 0, 0], [0, 0, 0, 0], [2, 0, 2, 0], [4, 4, 0, 0]]\n",
      "Score: 8\n",
      "Seeded board: [[2, 0, 0, 0], [0, 0, 0, 0], [2, 0, 2, 0], [4, 4, 0, 0]]\n",
      "[[0, 0, 0, 0], [0, 0, 0, 0], [4, 0, 2, 0], [4, 4, 2, 0]]\n",
      "Score: 12\n",
      "Seeded board: [[0, 0, 0, 0], [0, 0, 0, 0], [4, 0, 2, 0], [4, 4, 2, 0]]\n",
      "[[0, 0, 0, 0], [0, 0, 2, 0], [4, 2, 0, 0], [8, 2, 0, 0]]\n",
      "Score: 20\n",
      "Seeded board: [[0, 0, 0, 0], [0, 0, 2, 0], [4, 2, 0, 0], [8, 2, 0, 0]]\n",
      "[[0, 0, 0, 2], [0, 0, 0, 0], [4, 0, 0, 0], [8, 4, 2, 0]]\n",
      "Score: 24\n",
      "Seeded board: [[0, 0, 0, 2], [0, 0, 0, 0], [4, 0, 0, 0], [8, 4, 2, 0]]\n",
      "[[2, 0, 0, 0], [0, 0, 0, 0], [4, 0, 0, 0], [8, 4, 2, 2]]\n",
      "Score: 24\n"
     ]
    }
   ],
   "source": [
    "import logic_2048\n",
    "import time\n",
    "\n",
    "print(\"Initial board: \" + str(game_board))\n",
    "\n",
    "game_matrix = logic_2048.place_new(game_board)\n",
    "\n",
    "for each in range(10):\n",
    "    print(\"Seeded board: \" + str(game_matrix))\n",
    "\n",
    "    if each % 2:\n",
    "        game_matrix, success, top_value, total_score = logic_2048.transform_matrix(\n",
    "            game_matrix, \"a\", score\n",
    "        )\n",
    "    else:\n",
    "        game_matrix, success, top_value, total_score = logic_2048.transform_matrix(\n",
    "            game_matrix, \"s\", score\n",
    "        )\n",
    "    score = total_score\n",
    "\n",
    "    if logic_2048.win_check(game_matrix):\n",
    "        # If client wins, construct result json with status code 1 and send via websocket\n",
    "        print(\"CONGRATS! YOU WON!\")\n",
    "        print(str(game_matrix) + \"\\nScore: \" + str(score))\n",
    "\n",
    "    elif not success:\n",
    "        # If client loses, construct result json with status code 255 and send via websocket\n",
    "        print(\"Game over\")\n",
    "        print(str(game_matrix) + \"\\nScore: \" + str(score))\n",
    "    else:\n",
    "        print(str(game_matrix) + \"\\nScore: \" + str(score))\n",
    "        \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ChatGPT Version\n",
    "v\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Q-Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.inputs = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "        self.targets = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "\n",
    "        hidden_layer = tf.layers.dense(self.inputs, 128, activation=tf.nn.relu)\n",
    "        self.predictions = tf.layers.dense(hidden_layer, output_size)\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.square(self.targets - self.predictions))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implement the Deep Q-Learning Algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.001, discount_factor=0.99, exploration_rate=1.0, exploration_decay_rate=0.9995, memory_size=50000, batch_size=32):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.step = 0\n",
    "        self.model = DQN(input_size, output_size, learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return np.random.choice(range(self.output_size))\n",
    "        else:\n",
    "            Q_values = self.model.predictions.eval(feed_dict={self.model.inputs: [state]})[0]\n",
    "            return np.argmax(Q_values)\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def update_exploration_rate(self):\n",
    "        self.exploration_rate *= self.exploration_decay_rate\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([item[0] for item in batch])\n",
    "        actions = np.array([item[1] for item in batch])\n",
    "        rewards = np.array([item[2] for item in batch])\n",
    "        next_states = np.array([item[3] for item in batch])\n",
    "        dones = np.array([item[4] for item in batch])\n",
    "        next_Q_values = self.model.predictions.eval(feed_dict={self.model.inputs: next_states})\n",
    "        targets = rewards + self.discount_factor * np.max(next_Q_values, axis=1) * (1 - dones)\n",
    "        target_Q_values = self.model.predictions.eval(feed_dict={self.model.inputs: states})\n",
    "        target_Q_values[np.arange(len(actions)), actions] = targets\n",
    "        self.model.optimizer.run(feed_dict={self.model.inputs: states, self.model.targets: target_Q_values})\n",
    "\n",
    "        self.step += 1\n",
    "        if self.step % 100 == 0:\n",
    "            self.update_exploration_rate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}